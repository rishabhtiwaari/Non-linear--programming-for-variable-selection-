{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2490ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gurobipy as gp\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import linear_model\n",
    "\n",
    "RUNTIME = 600\n",
    "kfolds = KFold(n_splits=10)\n",
    "big_m = 100\n",
    "# Direct Variable Selection – MIQP Problem\n",
    "General form of Quadratic Optimization problem :\n",
    "\n",
    "Objective function : $x^{T}Qx + c^{T}x$ <br>\n",
    "such that <br> \n",
    "$ Ax \\le b $<br>\n",
    "$ x \\ge 0 $\n",
    "MIQL implementation has the below decision variables:\n",
    "1. $m+1$ betas - $ \\beta_{0}, \\beta_{1} ... \\beta_{m}$\n",
    "2. $m$ variables for variable selection $ z_{1}, z_{2} ... z_{m}$   <br>\n",
    "\n",
    "Total Constraint equations - 2m +1\n",
    "\n",
    "Objective function for MIQP : $$ \\beta^{T}(X^{T}X)\\beta + (-2y^{T}X)\\beta$$<br>\n",
    "such that <br> \n",
    "$$ -z_{i}M < \\beta_{i} < z_{i}M $$   $\\forall  i \\in [1,m]$<br>\n",
    "$$ \\sum_{i=1}^m z_{i} \\le k$$\n",
    "\n",
    "Comparing with the quadratic form we have :\n",
    "$Q =(X^{T}X)$<br>\n",
    "$c =(-2y^{T}X)$\n",
    "### Find the best k using MIQP\n",
    "lasso_d = {}\n",
    "k_lst = []\n",
    "sse_lst = []\n",
    "\n",
    "for k in [5,10,15,20,25,30,35,40,45,50]:\n",
    "    \n",
    "    train = pd.read_csv('training_data.csv')\n",
    "    X = np.array(train.iloc[0:,1:])\n",
    "    y = np.array(train['y'])\n",
    "    \n",
    "    sse = 0\n",
    "    \n",
    "    # cross validation\n",
    "    for train_index, holdout_index in kfolds.split(X):\n",
    "        X_train, X_holdout = X[train_index], X[holdout_index]\n",
    "        y_train, y_holdout = y[train_index], y[holdout_index]\n",
    "        \n",
    "        n = X_train.shape[0] # number of rows\n",
    "        m = X_train.shape[1] # number of variables\n",
    "\n",
    "        # quadratic term Q\n",
    "        Q = np.zeros((2*m+1, 2*m+1))\n",
    "        X_Q = np.zeros((n,m+1))\n",
    "        X_Q[0:n, 0] = 1\n",
    "        for i in range(n):        \n",
    "            X_Q[i, 1:] = X_train[i,:]\n",
    "        Q[:m+1,:m+1] = X_Q.T @ X_Q\n",
    "\n",
    "        # linear term C\n",
    "        C = np.zeros(2*m+1)\n",
    "        C[:m+1] = -2*(y_train.T) @ X_Q\n",
    "        \n",
    "        # create A matrix\n",
    "        A = np.zeros((2*m+1, 2*m+1))\n",
    "\n",
    "        # big M constraint: weights-M*binary  <= 0\n",
    "        np.fill_diagonal(A[:m, 1:m+1], 1) \n",
    "        np.fill_diagonal(A[:m, m+1:2*m+1], -big_m) \n",
    "        \n",
    "        # big M constraint: weights+M*binary  >= 0\n",
    "        np.fill_diagonal(A[m:-1, 1:m+1], 1) \n",
    "        np.fill_diagonal(A[m:-1, m+1:2*m+1], big_m) \n",
    "\n",
    "        # k stocks are chosen\n",
    "        A[-1, m+1:] = 1\n",
    "\n",
    "        sense = np.array(['<']*m + ['>']*m + ['<'])\n",
    "\n",
    "        b = np.concatenate((np.zeros(2*m), [k]))\n",
    "\n",
    "        lb = np.array([np.NINF]+[-big_m]*m+[np.NINF]*m)\n",
    "\n",
    "        ndxMod = gp.Model()\n",
    "        ndxMod_x = ndxMod.addMVar(len(Q),vtype=['C']*(m+1)+['B']*m, lb=lb) \n",
    "        ndxMod_con = ndxMod.addMConstrs(A, ndxMod_x, sense, b)\n",
    "        ndxMod.setMObjective(Q, C, 0, sense=gp.GRB.MINIMIZE)\n",
    "        ndxMod.Params.OutputFlag = 0\n",
    "        ndxMod.setParam('TimeLimit', RUNTIME)\n",
    "        ndxMod.optimize()\n",
    "        \n",
    "        # save the coefficients\n",
    "        coef = ndxMod_x.x[:m+1]\n",
    "        \n",
    "        n = X_holdout.shape[0] # number of rows\n",
    "        \n",
    "        # X variables for each record in the holdout set\n",
    "        X_Q = np.zeros((n,m+1))\n",
    "        X_Q[0:n, 0] = 1\n",
    "        for i in range(n):        \n",
    "            X_Q[i, 1:] = X_holdout[i,:]\n",
    "        \n",
    "        # calculate the SSE for holdout set\n",
    "        sse += np.transpose(X_Q @ coef - y_holdout) @ (X_Q @ coef - y_holdout)\n",
    "        \n",
    "    print(k,':',sse)\n",
    "    lasso_d[sse] = k\n",
    "    \n",
    "    k_lst.append(k)\n",
    "    sse_lst.append(sse)\n",
    "    \n",
    "best_k = lasso_d[min(k for k, v in lasso_d.items())] # save the best k\n",
    "print('The best number of X variables is:', best_k)\n",
    "### Plot the SSE\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.bar(k_lst, sse_lst, label='SSE')\n",
    "plt.title('Performance Evaluation')\n",
    "plt.xlabel('Number of X Variables')\n",
    "plt.ylabel('Sum of Square Errors')\n",
    "plt.legend()\n",
    "plt.show\n",
    "### Fit the MIQP model on the entire training set with k=10\n",
    "train = pd.read_csv('training_data.csv')\n",
    "\n",
    "X_train = np.array(train.iloc[0:,1:])\n",
    "y_train = np.array(train['y'])\n",
    "\n",
    "n = X_train.shape[0] # number of rows\n",
    "m = X_train.shape[1] # number of variables\n",
    "\n",
    "# quadratic term Q\n",
    "Q = np.zeros((2*m+1, 2*m+1))\n",
    "X = np.zeros((n,m+1))\n",
    "X[0:n, 0] = 1\n",
    "for i in range(n):        \n",
    "    X[i, 1:] = X_train[i,:]\n",
    "Q[:m+1,:m+1] = X.T @ X\n",
    "\n",
    "# linear term C\n",
    "C = np.zeros(2*m+1)\n",
    "C[:m+1] = -2*(y_train.T) @ X\n",
    "\n",
    "# create A matrix\n",
    "A = np.zeros((2*m+1, 2*m+1))\n",
    "\n",
    "# big M constraint: weights-M*binary  <= 0\n",
    "np.fill_diagonal(A[:m, 1:m+1], 1) \n",
    "np.fill_diagonal(A[:m, m+1:2*m+1], -big_m) \n",
    "        \n",
    "# big M constraint: weights+M*binary  >= 0\n",
    "np.fill_diagonal(A[m:-1, 1:m+1], 1) \n",
    "np.fill_diagonal(A[m:-1, m+1:2*m+1], big_m) \n",
    "\n",
    "# k stocks are chosen\n",
    "A[-1, m+1:] = 1\n",
    "\n",
    "sense = np.array(['<']*m + ['>']*m + ['<'])\n",
    "\n",
    "b = np.concatenate((np.zeros(2*m), [best_k]))\n",
    "\n",
    "lb = np.array([-big_m]*(m+1)+[0]*m)\n",
    "\n",
    "ndxMod = gp.Model()\n",
    "ndxMod_x = ndxMod.addMVar(len(Q),vtype=['C']*(m+1)+['B']*m, lb=lb) \n",
    "ndxMod_con = ndxMod.addMConstrs(A, ndxMod_x, sense, b)\n",
    "ndxMod.setMObjective(Q, C, 0, sense=gp.GRB.MINIMIZE)\n",
    "ndxMod.Params.OutputFlag = 0\n",
    "ndxMod.setParam('TimeLimit', RUNTIME)\n",
    "ndxMod.optimize()\n",
    "\n",
    "# save the coefficients\n",
    "coef = ndxMod_x.x[:m+1]\n",
    "        \n",
    "print(coef)\n",
    "### Make predictions of y_test using the saved coefficients\n",
    "test = pd.read_csv('test_data.csv')\n",
    "X_test = np.array(test.iloc[0:,1:])\n",
    "y_test = np.array(test['y'])\n",
    "\n",
    "n = X_test.shape[0] # number of rows\n",
    "m = X_test.shape[1] # number of variables\n",
    "\n",
    "# X variables for each record\n",
    "X = np.zeros((n,m+1))\n",
    "X[0:n, 0] = 1\n",
    "for i in range(n):        \n",
    "    X[i, 1:] = X_test[i,:]\n",
    "    \n",
    "X @ coef\n",
    "### Calaculate the MSE of y_test\n",
    "print('The MSE is:', mean_squared_error(y_test, X @ coef))\n",
    "# Indirect Variable Selection – LASSO\n",
    "### Fit a linear regression model using Lasso \n",
    "train = pd.read_csv('training_data.csv')\n",
    "X_train = np.array(train.iloc[0:,1:])\n",
    "y_train = np.array(train['y'])\n",
    "\n",
    "lasso = linear_model.LassoCV(cv=10).fit(X_train, y_train)\n",
    "\n",
    "print('Alpha value:', lasso.alpha_)\n",
    "print('Model intercept:', lasso.intercept_)\n",
    "print('Total X variables:', (lasso.coef_ != 0).sum())\n",
    "print('X variable coefficients:', lasso.coef_)\n",
    "### Make predictions of y_test\n",
    "test = pd.read_csv('test_data.csv')\n",
    "X_test = np.array(test.iloc[0:,1:])\n",
    "y_test = np.array(test['y'])\n",
    "\n",
    "lasso.predict(X_test)\n",
    "### Calculate the MSE of y_test\n",
    "print('The MSE is:', mean_squared_error(y_test, lasso.predict(X_test)))\n",
    "### EDA of comparison\n",
    "# predictions of y_test\n",
    "\n",
    "test = pd.read_csv('test_data.csv')\n",
    "X_test = np.array(test.iloc[0:,1:])\n",
    "y_test = np.array(test['y'])\n",
    "\n",
    "#lasso.predict(X_test)\n",
    "\n",
    "sns.kdeplot(X@coef,alpha=0.35,fill='orange',label='MIQP',legend=True)\n",
    "sns.kdeplot(lasso.predict(X_test),alpha=0.35,fill='skyblue',label='Lasso',legend=True)\n",
    "plt.legend()\n",
    "plt.xlabel('Prediction')plt.plot(coef[1:],alpha=0.55,label='MIQP',marker='.', markersize=10) \n",
    "plt.plot(lasso.coef_,alpha=0.55,label='Lasso',marker='.', markersize=10) #lets plot the second line\n",
    "plt.ylabel('Feature Index')\n",
    "plt.xlabel('Coefficient Value')\n",
    "plt.title('Comparison of regression coefficients')\n",
    "plt.legend(loc = 'best')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
